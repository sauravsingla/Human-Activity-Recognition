{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN.ipynb",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sauravsingla/Human-Activity-Recognition/blob/main/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8xQjklfkQpU"
      },
      "source": [
        "#import the libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SRjlM85EVV2"
      },
      "source": [
        "#opening the zip-file\n",
        "from zipfile import ZipFile\n",
        "file_name=\"UCI HAR Dataset.zip\"\n",
        "with ZipFile(file_name,'r') as zip:\n",
        "  zip.extractall()\n",
        "  print(\"DONE\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asgmbPmFEtNF"
      },
      "source": [
        "#those are separate normalised input features for the neural network\n",
        "SIGNALS = [\n",
        "    \"body_acc_x\",\n",
        "    \"body_acc_y\",\n",
        "    \"body_acc_z\",\n",
        "    \"body_gyro_x\",\n",
        "    \"body_gyro_y\",\n",
        "    \"body_gyro_z\",\n",
        "    \"total_acc_x\",\n",
        "    \"total_acc_y\",\n",
        "    \"total_acc_z\"\n",
        "]\n",
        "# Output classes to learn how to classify\n",
        "LABELS = [\n",
        "    \"WALKING\",\n",
        "    \"WALKING_UPSTAIRS\",\n",
        "    \"WALKING_DOWNSTAIRS\",\n",
        "    \"SITTING\",\n",
        "    \"STANDING\",\n",
        "    \"LAYING\"\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIeech3DEuaM"
      },
      "source": [
        "# Utility function to read the data from csv file\n",
        "def _read_csv(filename):\n",
        "    return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
        "\n",
        "# Utility function to load the load_signals\n",
        "def load_signals(subset):\n",
        "    signals_data = []\n",
        "\n",
        "    for signal in SIGNALS:\n",
        "        filename = f'/content/UCI HAR Dataset/{subset}/Inertial Signals/{signal}_{subset}.txt'\n",
        "        #/content/UCI HAR Dataset/train/Inertial Signals\n",
        "        #/content/UCI HAR Dataset/train/Inertial Signals/body_acc_x_train.txt\n",
        "        signals_data.append(\n",
        "            _read_csv(filename).to_numpy()\n",
        "        ) \n",
        "\n",
        "    # Transpose is used to change the dimensionality of the output,\n",
        "    # aggregating the signals by combination of sample/timestep.\n",
        "    # Resultant shape is (7352 train/2947 test samples, 128 timesteps, 9 signals)\n",
        "    return np.transpose(signals_data, (1, 2, 0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc4lx4DOE1C_"
      },
      "source": [
        "def load_y(subset):\n",
        "    filename = f'/content/UCI HAR Dataset/{subset}/y_{subset}.txt'\n",
        "    y = _read_csv(filename)[0]\n",
        "\n",
        "    return pd.get_dummies(y).to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zChTVcV7FAak"
      },
      "source": [
        "def load_data():\n",
        "    \"\"\"\n",
        "    Obtain the dataset from multiple files.\n",
        "    Returns: X_train, X_test, Y_train, Y_test\n",
        "    \"\"\"\n",
        "    X_train, X_test = load_signals('train'), load_signals('test')\n",
        "    Y_train, Y_test = load_y('train'), load_y('test')\n",
        "\n",
        "    return X_train, Y_train, X_test,  Y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gajiuY_PJVrv"
      },
      "source": [
        "# Loading the train and test data\n",
        "X_train, Y_train, X_test,  Y_test = load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dy4WII0cJIDN"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUdgHj3RFDzX"
      },
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "class scaling_tseries_data(BaseEstimator, TransformerMixin):\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    def __init__(self):\n",
        "        self.scale = None\n",
        "\n",
        "    def transform(self, X):\n",
        "        temp_X1 = X.reshape((X.shape[0] * X.shape[1], X.shape[2]))\n",
        "        temp_X1 = self.scale.transform(temp_X1)\n",
        "        return temp_X1.reshape(X.shape)\n",
        "\n",
        "    def fit(self, X):\n",
        "        # remove overlaping\n",
        "        remove = int(X.shape[1] / 2)\n",
        "        temp_X = X[:, -remove:, :]\n",
        "        # flatten data\n",
        "        temp_X = temp_X.reshape((temp_X.shape[0] * temp_X.shape[1], temp_X.shape[2]))\n",
        "        scale = StandardScaler()\n",
        "        scale.fit(temp_X)\n",
        "        self.scale = scale\n",
        "        return self"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qprVSQjFInCz"
      },
      "source": [
        "Scale = scaling_tseries_data()\n",
        "Scale.fit(X_train)\n",
        "X_train_sc = Scale.transform(X_train)\n",
        "X_test_sc = Scale.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMdLuvVuIoPs"
      },
      "source": [
        "print('Scaled_X_train',X_train_sc.shape)\n",
        "print('Scaled_X_test',X_test_sc.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GC6vZCnTJ1HN"
      },
      "source": [
        "**Base_Model of CNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yF-tBsHJ8Db"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv1D(filters=32, kernel_size=3, activation='relu',kernel_initializer='he_uniform',input_shape=(128,9)))\n",
        "model.add(Conv1D(filters=32, kernel_size=3, activation='relu',kernel_initializer='he_uniform'))\n",
        "model.add(Dropout(0.6))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(6, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Q8rHAgoKFWM"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRcYI48pKLe-"
      },
      "source": [
        "model.fit(X_train_sc,Y_train, epochs=45, batch_size=16,validation_data=(X_test_sc, Y_test), verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIYOJsUUK7bU"
      },
      "source": [
        "Y_pred = model.predict(X_test_sc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqHlFVZhLI6L"
      },
      "source": [
        "pred = np.argmax(Y_pred,axis = 1) \n",
        "Y_actual = np.argmax(Y_test,axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jffznYYdO9o9"
      },
      "source": [
        "confusion_matrix(Y_actual, pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mi7XUg9EPAT6"
      },
      "source": [
        "print(classification_report(Y_actual, pred))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}